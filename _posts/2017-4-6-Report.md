---
layout: post
title: Final Report (5/10 Draft)
description: A report on the progress so far
image: assets/images/engineering.jpg
---

This report covers our completed work so far, our analysis of the methods, as well as the details about our optimizations. We are still working on the final report!

## Completed Work

Generally, our attention has focused on three areas:

  * A quick, parallel quicksort in Go. 
  * A quick, parallel samplesort in Go.
  * A generic type support that is consistent with GoLang interface.

As we discussed in our original proposal, while this project was inspired by the sorting competition, it is more than just about building a fast method, but an analysis of the methods as well as the ways to optimize them. For example, quicksort is unlikely to be the best way for parallel (or sequential) sorting, but we spent a lot of time trying different optimizations on it (e.g. exploiting cache efficiency, etc.). This process has been very rewarding, as we are able to use many techniques and concepts learned in class to speed up a given method. 

We have also been focusing on the generic type support so that our library is completely consistent with Golang's requirement (see below). The competition is on an array of 100M pairs of `float64` values, but we have been focusing on some even more interesting objectives--- how to support the sorting using Go's builtin `sort.Interface` abstraction? As is to be introduced below, such interface limits the choice we have regarding the sorting methods, but also makes this project very interesting.


## Generic type support

A very very important aspect of our implementation is its generic type support. While the original competition test on 100M elements of type float64-pair, our implementation focuses on a more generic effort--- based on Go's `sort.Interface`. 

In particular, the sorting methods currently implemented in Go only accepts structures that implements this `sort.Interface` (see [here](https://golang.org/pkg/sort/#Interface) for official doc), which has only three operations available:

```go
type Interface interface {
        // Len is the number of elements in the collection.
        Len() int
        // Less reports whether the element with
        // index i should sort before the element with index j.
        Less(i, j int) bool
        // Swap swaps the elements with indexes i and j.
        Swap(i, j int)
}
```

This limits the sorting method we can possibly use. In particular, we are not able to know what the underlying data structure is or what is the type of the element (be it a string or integer or a pair of float64). We also don't have indexing (e.g. get and set methods), so we cannot do operations such as array copying and fetching. 

This makes it hard to implement the sorts. Despite the limitations/constraints introduced by supporting sorts based on this interface, we feel it is important because we want to make this consistent with Go, and eventually **(hopefully) make our project a handy Golang extension library**. 



## A study of Quicksort

#### Overview

Currently, QoSort library (our project) supports a version of optimized quicksort that outperforms all other versions of quicksort we have found in Go. The idea of quicksort is easy: recursively sorting the halves based on the pivots chosen. But there are truly two factors at stake here:

  1. How is the pivot chosen?
  2. How to schedule the tasks?

The answer to these two questions can be really easy or really hard. A naive way of implementation is simply picking the first element as pivot and then putting recursive tasks in goroutines (i.e. threads in Go) directly for sorting. 

However, these two factors turn out to be important. First, for large-size input, the pivot selection is especially important. A partition process usually takes a linear scan, so a bad pivot selection will incur not only a waste of a large amount of time (on a single core, scanning for partition), but also subsequent imbalanced load for recursive tasks.

Second, task scheduling is important. In general, there are four goals we considered critical in this project (and thus have been focusing on):

* Good cache locality for a single processor's task handling;
* Good load balance across processors at a given time;
* Little requirement on additional memory;
* Good scheduling of the tasks on subsequences on the processors.

Frankly, it's been a surprise to us that even an idea as "simple" as quicksort can lead to so many issues. An embarrassingly parallel version of sort with randomly picked pivot takes about 40 seconds to sort 100M elements on a 40-core machine, which is only slightly better than the result of sequential sort in GoLang (~65 seconds). 



#### Pivot Selection

As previously discussed, pivot selection is key to load balance and efficient execution, especially on large inputs. Our method to select pivot is primarily two-fold:

  1. If the sequence range to sort is sufficiently large, we pick 15 random elements for a series of "median of the three comparisons". In particular, the whole sequence is effectively broken into 3 equally large portions. 5 equi-distance points were picked. Finally, we choose the pivot by taking another "median of the three" among the medians of each portion.
  2. If the sequence is relatively small, we only pick 9 values to do pivot selection.

Our testing reveals such pivot selection to be relatively optimal ("optimal" as in close-to-median).



#### Scheduling

We have attempted different approaches to this problem. For example, we first implemented a push-based master-slave hierarchy where the sorting tasks are sent to a master, which acts as an auto-load scheduler that pushes the tasks to the appropriate workers. However, this requires $O(n)$ different channels for communication, and the unnecessary communications between cores tend to be a major overhead for the parallelization (although the performance is better than the naive parallel formulation).

So eventually, we built the following model that is able to best leverage the resources we have (see figure 1 & 2 below):

- A worker pool matching the # of cores available;
- A central task queue where tasks to be processed await;
- For each worker, once available, it should pull the next task from the queue, and then:
  1. the worker partitions the subsequence (i.e. its task), and tries to enqueue the smaller partition. If the queue is full, the worker will handle this smaller partition (by step 1 recursively) directly instead of being blocked. Moreover, it keeps breaking the largest partition and repeat this step (rather than pushing both partitions onto the queue).
  2. as long as the subsequence is smaller than some threshold value, serial sort is used.

<div class="imgcap">
<img src="/QoSort/assets/images/worker_pool_scheduling.png" style="width: 70%">
<div class="thecap" style="color: white; font-size: 14pt">Figure 1: Optimized quicksort's worker pool scheduling and task queue. The workers pull from a central queue, and push subtasks to it. In general, our method is able to guarantee good load balance!</div>
</div>

<br />

<div class="imgcap">
<img src="/QoSort/assets/images/qsort_worker_logic.png" style="width: 70%">
<div class="thecap" style="color: white; font-size: 14pt">Figure 2: Optimized quicksort's worker logic. Instead of pushing both tasks, recursing on one is more efficient, both space-wise and time-wise.</div>
</div>

<br />

There are some major advantages with this model:

  1. Avoid extra OS threading, whose context switch is relatively expensive. This is a larger problem if the input size is large--- quicksort is likely to spawn a number of goroutines, which recursively forks to other goroutines. 
  2. Low synchronization cost. Every worker processes a different region (i.e. independence); synchronization only needed at the end of the whole sorting.
  3. Relatively good cache locality. Instead of pushing both subsequences as tasks to the queue, we keep the larger portion and only push the smaller portion. This saves the effort for the worker to re-pull new tasks from the queue, and meanwhile have the worker deal with the very same subsequence. Better spatial locality!
  4. Balanced load. This is related to pivot selection as well--- since tasks are broken into approximately equal tasks, at a given time the tasks from the FIFO task queue should have tasks of almost same sizes (which we confirmed). 
  5. The independence of each worker's job means no extra memory is needed (and no need to copying). All of the operations can be completed in place. 

The results of the optimized quicksort have been very promising. For 100M float-64 elements on a 20-core machine (40 logical processors), for instance:

- **Builtin Golang's sort**: 65.4 seconds
- **Naive parallel quicksort** (embarrassingly parallel): 42.1 seconds
- **Push-based worker parallel quicksort**: 16.4 seconds
- **Optimized worker parallel quicksort**: 4.6 seconds

A further optimization: note that in the first pass, the partition requires a linear scan, which is expensive. But all the work happens on one core, with all other peers idle. Therefore, instead of doing a usual split-by-2, we optimize the model further by a split-by-3, so that we can push two tasks to the queue and better leverage the other cores we have. This further accelerates the sorting:

- **Further optimized worker parallel quicksort by split-by-3 initially**: 4.0 seconds

As was discussed above, our implementation is consistent with Golang's `sort.Interface`. This is very important, as we don't assume on the underlying data structure or element type. 

**The best baseline model we found that supports generic type was only able to sort 100M float64-pairs in about 7.3 seconds (a starred open source project)**, which is smaller than our result!



## Sample Sort

Our sample sort algorithm is developed base on the sample sort algorithm provided by Professor Guy Blelloch for the 210-sorting competetion, with many strategies adapted for the constraints of GoLang. We decided to proceed with Sample sort because it addresses all flaws with our parallel quick sort algorithm. First, the sampling step ensures input array are partitioned into more balanced sub-arrays, thus work is more balanced. Also upon analysis, we noticed a potential part of improvement for parallel quicksort was at the beginning of the algorithm. At the beginning of quicksort, there aren't many partitions for the worker threads to work on, so we are not able to utilize all the available cores at the beginning of quicksort. In sample sort, much of the work is parallelizable, even at the very first iteration of the algorithm. Also sample sort allows us to partition the array into many sub-arrays in the sampling step, which further improves core utilization.

<div class="imgcap">
<img src="/QoSort/assets/images/sampling_and_choose_pivot.png" style="width: 70%">
<div class="thecap" style="color: white; font-size: 14pt">Figure 3: Sampling and choosing pivots</div>
</div>

<br />

#### Sampling and Choosing Pivots
In this step, we select a sample set of elements from the input array. The size of the sample set is determined by the number of buckets we plan to use multiplied by a pre-defined over-sampling factor. The elements to be included in the sample set are chosen randomly. After the elements are chosen, we sort the sample set, and choose pivots from the sorted set. The pivots are chosen evenly distributed along the sorted samples, so the pivots have the best chance of dividing the input array into equally sized buckets.


#### Partition Blocks Using Pivots
With appropriate pivots chosen, it remains for us to partition the array into buckets to enable parallel sorting. To do this, we first partitioned the array into equal-sized blocks solely base on indices, then sorted elements within each block using our parallel quick sort algorithm. Since there's no dependency across blocks, Block sorting was carried out in parallel to achieve even better hardware utilization. With each block sorted, we can partition each block into small buckets, using the pivots chosen in the previous step.

<div class="imgcap">
<img src="/QoSort/assets/images/transpose_block_bucket.png" style="width: 70%">
<div class="thecap" style="color: white; font-size: 14pt">Figure 4: Transposing blocks into buckets</div>
</div>

<br />

#### Transposing Blocks into Buckets

Now we have each block partitioned into small buckets, the next step is to combine small buckets into large buckets and move them to their appropriate location on the array. This step turned out to be very demanding. In our first attempt, we followed the transpose step implemented in Prof. Blelloch's C++ sample sort, which uses parallel prefix sum (scan). However, when we implemented our version of parallel scan in GoLang, we learned that the overhead for parallel scan is way too much for our input size. After testing, we decided to keep a serial scan. After we obtained prefix sum results, combining buckets and moving data is perfectly suited for parallelism, since there is not interference between different sections of the array.

  


#### Sort Each Buckets Using Optimized QuickSort
After the transpose step, we have the array partitioned into similar sized buckets, with all elements in a bucket larger than elements in the previous buckets. All that left to do is to sort the elements within each bucket and we are done. Again, we use our parallel quicksort on each bucket, and the operation across different buckets are also carried out in parallel using a simple go routine and lambda function.
